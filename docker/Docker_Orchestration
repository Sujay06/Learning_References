CONTAINER-ORCHESTRATION:
------------------------
(Managing of multiple containers at the same time)

Orchestration services available::
1.Docker Swarm
2.Kubernetes
3.Apache Mesos
4.AWS ECS & EKS

Docker swarm services::
-----------------------
(Open ports 
	2377	#For cluster manangement
	7946	#For communication among nodes
	4789	#For overlay n/w traffic )

Create Swarm:(Creates Manager Node)
-------------
(Run in manager machine)::
------------------------
>docker swarm init		#Initialise the swarm and advertise the IP.
(OR)
>docker swarm init --advertise-addr 192.x.x.x
(Save locally the worker node swarm token from here)
>docker info			#Check whether the swarm is active or not
>docker node ls			#List the manager node and it's ID
(Run swarm join command in worker machine to join to the swarm)

Add Worker-node to the Mananger node::(Run in Worker machine)
-------------------------------------
>docker swarm join --token SWMTKN-1-49nj1cmql0jkz5s954yi3oex3nedyz0fb0xx14ie39trti4wxv-8vxv8rssmk743ojnwacrr2e7c 192.168.99.100:2377
(If this token is not available, run "docker swarm join-token worker" to get the token )
	Ps: NOw the worker node would have joined the swarm!!!...

Run in manager machine:
----------------------
>docker node ls		#To check the list of all the nodes in the swarm(lists both worker and manager node)

------------------------------------------------------------------------------------------------------------------------------------------------
To create a service and scale the service::(Run in manager)
------------------------------------------------------------
(TASK: A task is a container running in the service)

>docker service create --replicas 1 --name helloworld alpine ping docker.com		#Creates 1 service replica named "helloworld" using "alpine" image and runs the command "ping" at runtime of the container.
>docker service create --name antivirus --mode global -dt ubuntu	#To make the service global. Orchestrator runs this service as a task in all the nodes.
>docker service ls				#To list the running services

>docker service inspect helloworld		#This outputs the service list in JSON format.
>docker service inspect --pretty helloworld	#This outputs the service in readable format.
>docker service ps helloworld			#To list the nodes running in the service
>docker ps					#To see the details of the container running the task

>docker service scale helloworld=3		#Scales the helloworld service to 3 tasks.
>doker service scale <service01>=5 <service02>=10	#SCale command lets you scale more than 1 service at the same time
>docker service update --replicas=5 helloworld	#Scales the helloworld service to 5 tasks.Update can scale one service at a time.

>docker service ps helloworld			#Toview the scaled service containers
>docker ps					#To see all the running containers

>docker service rm helloworld			#Removes the helloworld service
>docker service inspect helloworld		#To verify the removal of the service.
>docker ps					#To verify that the containers are removed.

----------------------------------------------------------------------------------------------------------------------------------------------
Docker-Node Draining:
---------------------
(For Maintenance Use-Cases)
(Create mode than 2 worker nodes(Swarm01(Manager),Swarm02(Worker),Swarm03(Worker)))
>docker node ls							#To list all the running nodes
>docker service create --name mywebserver --replicas 5 nginx	#Create the service
>docker service ps mywebserver					#Check the running tasks in the service
>docker node update --availablity drain swarm03			#Mark Swarm03 as drain. This shuts down the Swarm03 node and all the tasks running in it are distributed between Swarm01 & Swarm02. Availabilty can be wither drain or active.
>docker service ps mywebserver					#Check the running tasks in the service
>docker node ls							#To check the status of the nodes
>docker node update --availability active swarm03		#To bring up the swarm03 and this distributes the tasks between all the 3 nodes.

>docker service inpsect <service-name>				#To list all the info regarding the service.Prints the outpt in JSON format.
>docker service inpsect --pretty <service-name>			#Outputs in readable format.

>docker node inspect <node-ID (OR) node-name>			#To list all the info regarding a node in JSON format.
>docker node inspect <node-ID (OR) node-name> --pretty		#Outputs in readable format.

>docker service create --name <service-name> --replicas <int> -p <int>:<int> <image>	#To publish a port for the internet to connect to the service.
	Eg: docker service create --name mywebserver --replicas 2 -p 8080:80 nginx
		>netsat -ntlp		#To check the port mappings
		>ifconfig --eth0	#Get the external IP.
		>curl <IP-Addr>:8080	#To see the nginx homepage

-----------------------------------------------------------------------------------------------------------------------------------------------
DOCKER-COMPOSE::
----------------
(On Linux machines you have to install docker-compose. On Mac & Windows it comes pre-installed)

To install on Linux Machines:
-----------------------------
>sudo curl -L "https://github.com/docker/compose/releases/download/1.25.0/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
>sudo chmod +x /usr/local/bin/docker-compose	#To change the permission
>docker-compose -v	#To check the version

Create a docker-compose.yml file
	(PS: YAML is indentation specific!!!)

>docker-compose config		#Lists any errors in the compose file, else displays the contents of the file
>docker-compose up -d 		#To initiate the compose
>docker ps			#To see the running containers in the compose
>docker-compose down		#To take down the compose

Docker-STACK-DEPLOY::
---------------------
>docker node ls			#To list all the running nodes
>docker-compose up -d
>docker stack deploy --compose-file docker-compose.yml <stack-name>	#To deploy the stack
   Eg: docker stack deploy --compose-file docker-compose.yml mydemo
>docker stack ps mydemo		#To check the status of the stack
>docker stack rm mydemo		#To remove the stack

------------------------------------------------------------------------------------------------------------------------------------------------
Locking Swarm Cluster::
-----------------------
>docker node ls			#To list the runnig nodes in the swarm

	To view the private key for the docker swarm::
		cd /var/lib/docker
		cd swarm
		cd certificates
		cat swarm-node-key

>docker swarm update --autolock=true	#To enable the swarm lock feature.This provides the lock-key.Save this key to unlock the swarm in future
>systemctl restart docker		#Restart the docker daemon for the docker swarm lock to take place
>docker node ls				#This basically says that swarm is locked 
>docker swarm unlock 			#Pass the lock-key here
>docker node ls				#This lists the unlocked-running-nodes in the swarm
>docker swarm unlock-key		#To retrieve the unlock-key
>docker swarm unlock-key --rotate	#To rotate the key
>docker swarm update --autolock=false	#To disable the swarm locking feature

Mounting Volumes via Swarm::
----------------------------
>docker service create --name myservice --mount type=volume,source=myvol,target=/mypath nginx		#Create a service named myservice from nginx image and mount "myvol" volume specified in the path /mypath.
>docker service ps myservice		#To check in which swarm node the volume is created and mounted.
(Go to the particular swarm node)
>docker ps				#To list the running task
>docker volume ls			#To list the volumes in the node
>docker exec -it <Container-ID> bash	#Enter into the container
(Create a file called test.txt in "myvol" and exit the container)
>ls -l					
>cd /mypath/
>touch test.txt
>exit
>cd /var/lib/docker/volumes/myvol/_data
>ls
(Should display test.txt)
(Delete the service and check whether the Volume is removed or not)
>docker service rm myservice		#To remove the service
>docker volume ls			#To check whether myvol still persists or not
>ls -l					#TO list test.txt

-------------------------------------------------------------------------------------------------------------------------------------------------
CONTROL-SERVICE-PLACEMENTS::
----------------------------
Following placement constraints can be applied on docker-swarm:
1.Replicated or Global Services
2.Resource Constraints(CPU & Memory)
3.Placement Constraints(Only run on nodes with label "pci_compliance=true")
4.Placement Preferences

(Eg here based on placement preferences(Assuming there are 3 nodes in the swarm i.e Swarm01,02,03))

To add label to a specific node:
> docker node update --label-add <label>-<value> <node-ID>
	Eg: docker node update --label-add region-blr <node-ID>
>docker node inspect <node-ID>		#To check for applied label and passed values.

>docker service create --name myservice --constraint node.labels.region==blr --replicas 2 nginx		#This creates  2 replicas of myservice from nginx image with placement constraints placed as "blr".Service will be launched in node with label as "region" with value as "blr"
>docker service ps myservice		#To verify that all the tasks are started in the same node

>docker node ls				#To get the node-ID
>docker node inspect <node-ID>		#To get info on the node and to check for the labels in this case.


Creating CUSTOM OVERLAY networks for DOCKER-SWARM::
-------------------------------------------
(Docker Swarm run in Overlay n/w. This allows inter-node communication)

>docker network create --driver overlay myoverlay		#To create a new overlay n/w named myoverlay
>docker network ls						#To list all the n/ws
>docker service create --name myservice --network myoverlay --replicas 2 nginx		#To create 2 nginx replicas of service named myservice in the newly created myoverlay network.
>docker service ps myservice			#To list the running serices.
>docker ps 					#To get the container-ID
>docker container inspect <container-id>	#To inspect the container and verify the overlay n/w

To  remove Service and Network:
>docker service rm mynetwork			#To remove the service
>docker network rm myoverlay			#To remove the network

To ENCRYPT the OVERLAY N/W::
----------------------------
>docker network create --opt encrpyted --driver overlay my_secure_overlay	#This creates a encrypted overlay n/w.When you run services in this n/w, IPSEC tunnels are created between the nodes in the swarm which provides secure communication.
	NOTE:
	-----	>IPSEC tunnels are created when n/w is created and services are run in it.
		>This method uses AES 256 algorithm in GCM mode.
		>Manager automatically rotates the ecryption key every 12hrs.
		>Overlay n/w encryption is not supported in Windows. If windows node attemts to communicate in this n/w, no error is detected but the node will not be able to communicate.

------------------------------------------------------------------------------------------------------------------------------------------------
SERVICE CREATION USING TEMPLATES::
---------------------------------
>docker service create --name <service-name> --hostname="{{.Node.Hostname}}-{{.Service.Name}}" <image>	 #Here, --hostname is called the flag, ".Node.Hostname" & ".Service.Name" are called as place-holders.

	Eg: docker service cerate --name myservice --hostname="{{.Node.Hostname}}-{{.Service.Name}}" nginx		#Create the service from the nginx image naming it myservice, pass the node name(say swarm01) as the hostname in ".Node.Hostname" and pass the service name(say myservice) as name of the service in ".Service.Name"
	   
	NOTE: EXEC into the container and run "hostname" to view the output as swarm01-myservice.(PS:assuming the node name here is Swarm01)
		
	Following flags are allowed:
		--hostname
		--mount
		--env
	Following place-holders are allowed:
		.Service.ID		.Node.ID		.Task.ID
		.Service.Name		.Node.Hostname		.Task.Name
		.Service.Labels					.Task.Slot

------------------------------------------------------------------------------------------------------------------------------------------------
SPLIT-BRAIN and QUORUM::
------------------------
>Swarm manager nodes use the Raft Consensus Algorithm to manage the swarm state. 
>There is no limit on the number of manager nodes.
>The decision about how many manager nodes to implement is a trade-off between performance and fault-tolerance. Adding manager nodes to a swarm makes the swarm more fault-tolerant. However, additional manager nodes reduce write performance because more nodes must acknowledge proposals to update the swarm state. This means more network round-trip traffic.
>Raft requires a majority of managers, also called the quorum, to agree on proposed updates to the swarm, such as node additions or removals. 
>If a swarm loses the quorum of managers, swarm tasks on existing worker nodes continue to run. However, swarm nodes cannot be added, updated, or removed, and new or existing tasks cannot be started, stopped, moved, or updated.
>Use (N-1)/2 (where N is the number of Manager-Nodes in the cluster) formula to determine the number of faulty nodes that the cluster can handle.

HIGH AVAILABILITY OF DOCKER-MANAGER-NODES::
-------------------------------------------
>docker swarm join-token manager	#Get the token from here and run it in other nodes to make them managers and join the swarm.
	NOTE:ALWAYS USE ODD NUMBER OF MANAGER NODES(3,5,7)

>Distribute the manager nodes across 3-AZs to have best fault-tolerant cluster

Swarm manager nodes 	Repartition (on 3 Availability zones)
	3 			1-1-1
	5 			2-2-1
	7 			3-2-2
	9 			3-3-3


RUNNING MANAGER-ONLY NODE::
---------------------------
>By default, manager also acts as a worker node i.e tasks from the services would also be created in manager node. However, this approach is not recommended as it consumes CPU, Memory, I/O and ideally manager node should only perform managerial operations.

	To avoid this scenario, drain the manger node once the service is created.
>docker swarm init --advertise-addr <IP>
	Get the swarm token for workers and run it in worker nodes.
>docker node ls
>docker service create --name myservice --replicas 5 nginx
>docker node ps myservice
>docker node update --availability drain < node-ID>
	This will load balance the tasks amongst different worker nodes and the manager's resource would be available for managerial tasks!!.

RECOVERING FROM LOST QUORUM::
-----------------------------
When the quorum is lost and the leader of the manager nodes is inactive,run the following commands in one of the active manager nodes.
>docker swarm init --force-new-cluster --advertise-addr <IP>	#Pass the IP which was previously advertised.
	Cluster data is retrievable because all the manager node data is stored in "Internal Distributed State Store" for manager nodes.

-----------------------------------------------------------------------------------------------------------------------------------------------
DOCKER-SYSTEM COMMANDS::
-------------------------
>docker system df	#To check the disk space utilization
>docker system df -v

>docker system info	#Captures all the info regarding the docker host

>docker system events	#Basically for logs capturing. Works on Containers, Images,Volumes, Networks & Docker-daemon
				NOTE:Read the docs for more info. This is important!!!....
>docker system prune

-----------------------------------------------------------------------------------------------------------------------------------------------

Swarm initialized: current node (w49iulhgs2i6n4ul0iawsbhb8) is now a manager.

To add a worker to this swarm, run the following command:

    docker swarm join --token SWMTKN-1-663vhrppl2l4vn4dmz7r1w0ak60r1hm4yb0orz7wmsfa60u2fg-7mlwa5emx8d1m7eok65mynd63 172.31.41.2:2377

To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.




